{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyTorch_high_level_apis.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"pKVeLItnS5uV"},"source":["Copyright (C) 2020 Software Platform Lab, Seoul National University\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\"); \n","\n","you may not use this file except in compliance with the License. \n","\n","You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 \n","\n","Unless required by applicable law or agreed to in writing, software \n","\n","distributed under the License is distributed on an \"AS IS\" BASIS, \n","\n","\n","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n","\n","\n","See the License for the specific language governing permissions and\n","\n","\n","limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"2LQHVwI1g_PF"},"source":["### Preparing MNIST Dataset\n","\n","Pytorch is providing a MNIST Dataset class, so we can simply import it from `torchvision.datasets` without handcrafting it. Before instantiating Dataset objects, let's first define transforms for MNIST dataset. `torchvision.transforms` module contains multiple predefined transforms. Here, we apply `ToTensor()` that converts the data into the PyTorch Tensor type and `Normalize(mean, std)` that normalizes the sample data to have given mean and standard deviation. "]},{"cell_type":"code","metadata":{"id":"clLlHNHGz36J"},"source":["import torchvision.transforms as transforms\n","\n","# Normalize data with mean=0.5, std=1.0\n","mnist_transform = transforms.Compose([\n","    transforms.ToTensor(), \n","    transforms.Normalize((0.5,), (1.0,))\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cOUK49KAz36N"},"source":["We use `torchvision.datasets.MNIST` API to instantiate MNIST Dataset objects. Having `download=True` flag, MNIST dataset will be automatically downloaded at `download_root` unless it already exists."]},{"cell_type":"code","metadata":{"id":"LydTwAzMhHw0"},"source":["from torchvision.datasets import MNIST\n","\n","# download path\n","download_root = './MNIST_DATASET'\n","\n","train_dataset = MNIST(download_root, transform=mnist_transform, train=True, download=True)\n","test_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jE6MBKTYz36R"},"source":["Finally, we instantiate DataLoader that can shuffles and batches the MNIST Dataset. "]},{"cell_type":"code","metadata":{"id":"g_E6ELImz36S"},"source":["from torch.utils.data import DataLoader\n","\n","BATCH_SIZE = 64\n","\n","train_loader = DataLoader(dataset=train_dataset, \n","                         batch_size=BATCH_SIZE,\n","                         shuffle=True)\n","\n","test_loader = DataLoader(dataset=test_dataset, \n","                         batch_size=BATCH_SIZE,\n","                         shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QACEGZ2KAR7m"},"source":["## Custom Neural Network Models \n","\n","PyTorch `torch.nn.Module` allows you to easily make your own custom neural network model. All you need to do is to 1) make a class that inherits `torch.nn.Module` class and 2) define `forward` method. Let's build a simple CNN model using `torch.nn` APIs. \n","\n","* `torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')` \n","* `torch.nn.Linear(in_features, out_features, bias=True)`\n","* `torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)`\n","* `torch.nn.functional.relu(input, inplace=False)`\n","* `torch.nn.functional.softmax(input, dim=None)`\n","\n","You can refer to the following links for more detailed descriptions of `torch.nn` APIs.\n","* https://pytorch.org/docs/stable/nn.html\n","* https://pytorch.org/docs/stable/nn.functional.html"]},{"cell_type":"code","metadata":{"id":"izGiUAELAR7n"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","class Net(nn.Module):\n","  \n","    def __init__(self):\n","        super(Net, self).__init__()\n","        \n","        self.conv1 = nn.Conv2d(1, 6, 5, 1)\n","        self.pool1 = nn.MaxPool2d(2)\n","\n","        self.conv2 = nn.Conv2d(6, 16, 5, 1)\n","        self.pool2 = nn.MaxPool2d(2)\n","        \n","        self.fc1 = nn.Linear(256, 64)\n","        self.fc2 = nn.Linear(64, 10)\n","    \n","    def forward(self, x):\n","    \n","        # First convolution layer\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.pool1(x)\n","        \n","        \n","        # Second convolution layer\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = self.pool2(x)\n","        \n","        # (N, 256)\n","        x = x.view(-1, 256)\n","        \n","        # First fully-connected layer\n","        x = F.relu(self.fc1(x))\n","        \n","        # Second fully-connected layer\n","        x = self.fc2(x)\n","    \n","        return F.softmax(x, dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9OJowVblz36Z"},"source":["Instantiate the custom neural network model."]},{"cell_type":"code","metadata":{"id":"NQSRf32Zz36Z"},"source":["net = Net()\n","print(net)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-FenyD3rz36d"},"source":["Define the loss function (`criterion`) and the optimization method (`optimizer`). In this example, cross entropy loss is used as the criterion and SGD is used as the optimizer. By having `net.parameters()` as the input for the optimizer, we are trying to apply SGD to all the trainable parameters that consist of our custom neural network model `net`."]},{"cell_type":"code","metadata":{"id":"N8b4ed4Xz36e"},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.01)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6nd6Js7Nz36h"},"source":["Finally, connecting altoghether, we can train the model."]},{"cell_type":"code","metadata":{"id":"jiGgTcMrz36i"},"source":["num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    train_loss = 0.0\n","    # Iteration over the train dataset\n","    for i, data in enumerate(train_loader):\n","        x, label = data\n","        # 1. Initalize gradient values \n","        optimizer.zero_grad()\n","        # 2. Forward propagation\n","        model_output = net(x)\n","        # 3. Calculate loss using the criterion\n","        loss = criterion(model_output, label)\n","        # 4. Back propagation \n","        loss.backward()\n","        # 5. Weight update\n","        optimizer.step()\n","        \n","        train_loss += loss.item()\n","        \n","    # Print train loss and test accuracy at the end of every epoch   \n","    with torch.no_grad(): # do not forget this\n","        corr_num = 0\n","        total_num = 0\n","        # Iteration overt the test dataset to evaluate the test accuracy\n","        for _, test in enumerate(test_loader):\n","            test_x, test_label = test\n","            test_output = net(test_x)\n","            pred_label = test_output.argmax(dim=1)\n","            corr = test_label[test_label == pred_label].size(0)\n","            corr_num += corr\n","            total_num += test_label.size(0)\n","    print(\"[Epoch: %d] train loss: %.4f, test acc: %.2f\" \\\n","        % (epoch + 1, train_loss / len(train_loader), corr_num / total_num * 100))\n","    train_loss = 0.0\n","            "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B-N6izYSXFoU"},"source":[""],"execution_count":null,"outputs":[]}]}