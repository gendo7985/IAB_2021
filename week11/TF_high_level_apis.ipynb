{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwlPAA8ZJUsr"
   },
   "source": [
    "Copyright (C) 2019 Software Platform Lab, Seoul National University\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); \n",
    "\n",
    "you may not use this file except in compliance with the License. \n",
    "\n",
    "You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 \n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software \n",
    "\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS, \n",
    "\n",
    "\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n",
    "\n",
    "\n",
    "See the License for the specific language governing permissions and\n",
    "\n",
    "\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sO8dQBJs5Yxh"
   },
   "source": [
    "## Defining a model in TensorFlow \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZV9KmwLR_wBQ"
   },
   "source": [
    "In TensorFlow, various libraries regarding the model definition are provided under `tf.keras`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqWzzFUO79nx"
   },
   "source": [
    "### Model Subclassing\n",
    "We can build a fully-customizable model by subclassing [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model) and defining your own forward pass. Layers are created in the `__init__` method, provided by the [tf.keras.layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers)  and they are set as attributes of the class instance. The forward pass is defined in the `call` method. You can access model variables by `model.trainable_variables`.\n",
    "\n",
    "Below is an example of a linear regression model to be defined as a subclass of `tf.keras.Model`, and then be trained using loss function, gradient function and optimizer provided in [tf.keras.optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers). Useful loss functions are also provided in [tf.keras.losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses). We will cover these in more detail as we go on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zu-X05yCDq9V"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aRHEy317_-s0"
   },
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 2000\n",
    "toy_inputs = tf.random.normal([NUM_EXAMPLES, 1])\n",
    "noise = tf.random.normal([NUM_EXAMPLES, 1])\n",
    "toy_outputs = toy_inputs * 2 - 1 + noise * 1/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Uy1x6VF-_-s1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 1.325\n",
      "Trainable variables:\n",
      "\t toy_model/dense/kernel:0 :  [[1.466232]]\n",
      "\t toy_model/dense/bias:0 :  [0.]\n"
     ]
    }
   ],
   "source": [
    "class ToyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        \"\"\"Define layers\"\"\"\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.dense = tf.keras.layers.Dense(units=1)\n",
    "\n",
    "    def call(self, input):\n",
    "        \"\"\"Define forward pass.\"\"\"\n",
    "        result = self.dense(input)        \n",
    "        return result\n",
    "\n",
    "\n",
    "# The loss function to be optimized (MSE loss)\n",
    "def loss(model, inputs, targets):\n",
    "    error = model(inputs) - targets\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "model = ToyModel()\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model, toy_inputs, toy_outputs)))\n",
    "print(\"Trainable variables:\")\n",
    "for var in model.trainable_variables:\n",
    "  print(\"\\t\", var.name, \": \", var.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QZ3w_EmG7shn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 000: 1.275\n",
      "Loss at step 020: 0.607\n",
      "Loss at step 040: 0.306\n",
      "Loss at step 060: 0.170\n",
      "Loss at step 080: 0.109\n",
      "Loss at step 100: 0.082\n",
      "Loss at step 120: 0.069\n",
      "Loss at step 140: 0.064\n",
      "Loss at step 160: 0.061\n",
      "Loss at step 180: 0.060\n",
      "Loss at step 200: 0.060\n",
      "Loss at step 220: 0.059\n",
      "Loss at step 240: 0.059\n",
      "Loss at step 260: 0.059\n",
      "Loss at step 280: 0.059\n",
      "Final loss: 0.059\n",
      "Trainable variables:\n",
      "\t toy_model/dense/kernel:0 :  [[2.0022507]]\n",
      "\t toy_model/dense/bias:0 :  [-0.99436325]\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for i in range(300):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, toy_inputs, toy_outputs)\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(model, toy_inputs, toy_outputs)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(model, toy_inputs, toy_outputs)))\n",
    "print(\"Trainable variables:\")\n",
    "for var in model.trainable_variables:\n",
    "  print(\"\\t\", var.name, \": \", var.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8MEnHhAvskB"
   },
   "source": [
    "It's not required to set an input shape for the `tf.keras.Model` class since the parameters are set the first time input is passed to the layer.\n",
    "\n",
    "tf.keras.layers classes create and contain their own model variables that are tied to the lifetime of their layer objects. To share layer variables, share their objects.\n",
    "\n",
    "Below examples shows a new model that relies on the previous toy model. We are going to employ an additional bias to fit a slightly different data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eFF5jQ___-s7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 9.045\n",
      "Trainable variables:\n",
      "\t toy_model/dense/kernel:0 :  [[2.0022507]]\n",
      "\t toy_model/dense/bias:0 :  [-0.99436325]\n",
      "\t another_bias:0 :  0.0\n"
     ]
    }
   ],
   "source": [
    "toy_outputs_2 = toy_outputs + 3\n",
    "\n",
    "class ToyModel2(tf.keras.Model):\n",
    "    def __init__(self, toy_model):\n",
    "        \"\"\"Define layers\"\"\"\n",
    "        super(ToyModel2, self).__init__()\n",
    "        self.toy_model = toy_model\n",
    "        self.b = tf.Variable(0., name='another_bias')\n",
    "\n",
    "    def call(self, input):\n",
    "        \"\"\"Define forward pass.\"\"\"\n",
    "        result = self.toy_model(input)        \n",
    "        return result + self.b\n",
    "\n",
    "\n",
    "model2 = ToyModel2(model)\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model2, toy_inputs, toy_outputs_2)))\n",
    "print(\"Trainable variables:\")\n",
    "for var in model2.trainable_variables:\n",
    "  print(\"\\t\", var.name, \": \", var.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZf0cTk7_-s9"
   },
   "source": [
    "We are only optimizing the additional bias. The weight and bias of toy_model_1 does not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PhNlE5mTw7bX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 000: 8.689\n",
      "Loss at step 020: 3.906\n",
      "Loss at step 040: 1.774\n",
      "Loss at step 060: 0.823\n",
      "Loss at step 080: 0.400\n",
      "Loss at step 100: 0.211\n",
      "Loss at step 120: 0.127\n",
      "Loss at step 140: 0.089\n",
      "Loss at step 160: 0.073\n",
      "Loss at step 180: 0.065\n",
      "Loss at step 200: 0.062\n",
      "Loss at step 220: 0.060\n",
      "Loss at step 240: 0.060\n",
      "Loss at step 260: 0.060\n",
      "Loss at step 280: 0.059\n",
      "Final loss: 0.059\n",
      "Trainable variables:\n",
      "\t toy_model/dense/kernel:0 :  [[2.0022507]]\n",
      "\t toy_model/dense/bias:0 :  [-0.99436325]\n",
      "\t another_bias:0 :  2.9905908\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for i in range(300):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model2, toy_inputs, toy_outputs_2)\n",
    "    grads = tape.gradient(loss_value, [model2.b]) # gradient w.r.t. `model2.b`, not `model2.trainable_variables`\n",
    "    optimizer.apply_gradients(zip(grads, [model2.b]))# optimize only `model2.b`\n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(model2, toy_inputs, toy_outputs_2)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(model2, toy_inputs, toy_outputs_2)))\n",
    "print(\"Trainable variables:\")\n",
    "for var in model2.trainable_variables:\n",
    "  print(\"\\t\", var.name, \": \", var.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjAodT-v_-r7"
   },
   "source": [
    "## Convolutional Neural Networks\n",
    "Build simple CNN in TensorFlow.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LQHVwI1g_PF"
   },
   "source": [
    "### Preparing MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LydTwAzMhHw0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (60000,)\n",
      "(10000, 28, 28, 1) (10000,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Download the mnist dataset using keras\n",
    "data_train, data_test = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Parse images and labels\n",
    "(train_images, train_labels) = data_train\n",
    "(test_images, test_labels) = data_test\n",
    "\n",
    "# Numpy reshape & type casting\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_labels = train_labels.astype('int64')\n",
    "test_labels = test_labels.astype('int64')\n",
    "\n",
    "\n",
    "# Normalizing the images to the range of [0., 1.]\n",
    "train_images /= 255.\n",
    "test_images /= 255.\n",
    "\n",
    "print(train_images.shape, train_labels.shape)\n",
    "print(test_images.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJ5XuFPCBOZR"
   },
   "source": [
    "### Define the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "i_USTku5_-r8"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "# Construct a tf.keras.model using tf.keras\n",
    "class MyCNN(Model):\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='valid')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='valid')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='valid')\n",
    "        self.maxpool = tf.keras.layers.MaxPooling2D((2, 2))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "    \n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = MyCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yk4Cg8gRpWH_"
   },
   "source": [
    "### Setting up training\n",
    "After the model is constructed, we specify optimizer and loss function. We can also monitor training using metrics:\n",
    "* `optimizer`: This field specifies which optimizer to use. We can pass an optimizer instance (e.g., `tf.keras.optimizers.Adam`, `tf.keras.optimizers.RMSProp`), which are defined in  `tf.train` module.\n",
    "* `loss`: The function to minimize during optimization. Common choices include `mean square error (mse)`, `[categorical|binary]_crossentropy`. Loss functions are specified by name or by passing a callable object from the `tf.keras.losses` module.\n",
    "* `metrics`: Used to monitor training. We can put string names or callables defined in `tf.keras.metrics` module (e.g. `'accuracy'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6cBrLLCDpq2a"
   },
   "outputs": [],
   "source": [
    "# Choose loss function and optimizer for training\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Metrics to measure loss and accuracy of the model\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXKIy_UECa7P"
   },
   "source": [
    "### Train and Test functions using `tf.function`\n",
    "By annotating a train function with `tf.function`, TensorFlow internally creates a graph so that it can benefit from graph-based execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KUZYje1bC1xB"
   },
   "outputs": [],
   "source": [
    "# Define function for training\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_fn(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "# Define function for testing\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images, training=False)\n",
    "    loss = loss_fn(labels, predictions)\n",
    "\n",
    "    test_loss(loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDilJK1dC4f_"
   },
   "source": [
    "### Prepare the dataset and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "797JZG-zDBFa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 Loss = 0.2855 Train accuracy = 0.9139 Test loss = 0.0813 Test accuracy = 0.9755\n",
      "Epoch: 02 Loss = 0.0866 Train accuracy = 0.9727 Test loss = 0.0560 Test accuracy = 0.9826\n",
      "Epoch: 03 Loss = 0.0611 Train accuracy = 0.9810 Test loss = 0.0486 Test accuracy = 0.9853\n",
      "Epoch: 04 Loss = 0.0474 Train accuracy = 0.9849 Test loss = 0.0524 Test accuracy = 0.9845\n",
      "Epoch: 05 Loss = 0.0389 Train accuracy = 0.9872 Test loss = 0.0551 Test accuracy = 0.9828\n",
      "Epoch: 06 Loss = 0.0318 Train accuracy = 0.9898 Test loss = 0.0484 Test accuracy = 0.9855\n",
      "Epoch: 07 Loss = 0.0283 Train accuracy = 0.9911 Test loss = 0.0514 Test accuracy = 0.9851\n",
      "Epoch: 08 Loss = 0.0239 Train accuracy = 0.9919 Test loss = 0.0511 Test accuracy = 0.9847\n",
      "Epoch: 09 Loss = 0.0197 Train accuracy = 0.9939 Test loss = 0.0529 Test accuracy = 0.9855\n",
      "Epoch: 10 Loss = 0.0163 Train accuracy = 0.9947 Test loss = 0.0448 Test accuracy = 0.9887\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# Prepare the dataset using tf.data\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_ds = train_ds.shuffle(10000)\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "test_ds = test_ds.batch(batch_size)\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at each epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for images, labels in test_ds:\n",
    "        test_step(images, labels)\n",
    "\n",
    "    print('Epoch: %02d' % (epoch + 1),\n",
    "          'Loss = {:2.4f}'.format(train_loss.result()),\n",
    "          'Train accuracy = {:2.4f}'.format(train_accuracy.result()),\n",
    "          'Test loss = {:2.4f}'.format(test_loss.result()),\n",
    "          'Test accuracy = {:2.4f}'.format(test_accuracy.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7f9KSx3D9kF"
   },
   "source": [
    "## More simplified process using Keras API\n",
    "Keras API provides much simpler version to define a model and train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvCIHyMGEPEo"
   },
   "source": [
    "### Defining a model\n",
    "Let's take a look how we can define a model using Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "j8PUmEq5rO44"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "# Let's build a stack of *sequential* layers, which is\n",
    "# the most common form of neural network graphs.\n",
    "model = models.Sequential()\n",
    "\n",
    "# Adds a reshaping layer that transforms (28, 28, 1) to (784,)\n",
    "model.add(layers.Reshape((784,), input_shape=(28, 28, 1)))\n",
    "\n",
    "# Adds a dense layer with 128 units to the model\n",
    "model.add(layers.Dense(units=128, activation='relu'))\n",
    "\n",
    "# Adds another layer, which has L2 regularization applied to the kernel matrix\n",
    "model.add(layers.Dense(units=64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "\n",
    "# Adds a dense layer with 10 output units\n",
    "model.add(layers.Dense(units=10, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJq7I5kOc1H-"
   },
   "source": [
    "### Setting up training\n",
    "After the model is constructed, `compile` method configures how to learn the model, by specifying optimizer, loss function and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Ocskyx96c0UY"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSo7qrwZlOrl"
   },
   "source": [
    "### Training a model\n",
    "We can train the model using the `fit` method and then the model is \"fit\" to the training data. We can specify the training data to use (`images_train` and `labels_train`), how many epochs we will run (`epochs`), and how many items to be processed in a batch (`batch_size`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nPOV-4VXk53s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.6714 - accuracy: 0.9033\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.2678 - accuracy: 0.9485\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.1912 - accuracy: 0.9605\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.1572 - accuracy: 0.9668\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.1363 - accuracy: 0.9715\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.1220 - accuracy: 0.9744\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.9769\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.1009 - accuracy: 0.9793\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0934 - accuracy: 0.9808\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0877 - accuracy: 0.9823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2800df37c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymPOEP3BlivE"
   },
   "source": [
    "### Evaluating the model\n",
    "Finally, we evaluate the trained model using test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "yV5w-V99l0Di"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.1093 - accuracy: 0.9754\n",
      "Test accuracy: 0.9753999710083008\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4ORwtammNNb"
   },
   "source": [
    "### **Quiz**\n",
    "First, define a multi-layer model using Keras API following the CNN model defined in the beginning.\n",
    "\n",
    "The model should contain at least 3 convolutional layers, 3 max pooling layers, and 1 dense layer.\n",
    "\n",
    "The test accuracy after training should be higher than 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "lhKpYAgszdkI"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "############# Write here. #############\n",
    "class MyCNN(Model):\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='valid')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='valid')\n",
    "        self.conv3 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='valid')\n",
    "        self.maxpool = tf.keras.layers.MaxPooling2D((2, 2))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = MyCNN()\n",
    "#######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmnL8eeL613b"
   },
   "source": [
    "Using the model and `(train_images, train_labels)` above, let's train the model using the following configuration:\n",
    "* optimizer: `tf.keras.optimizers.Adam`\n",
    "* learning rate: 0.001\n",
    "* loss: `SparseCategoricalCrossentropy`\n",
    "* metrics: `accuracy`\n",
    "* batch size: 128\n",
    "* epochs: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "B0tNzWLWz0bj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zendo/anaconda3/lib/python3.8/site-packages/keras/backend.py:4906: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 8s 16ms/step - loss: 0.2894 - accuracy: 0.9159\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 8s 16ms/step - loss: 0.0832 - accuracy: 0.9748\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 8s 16ms/step - loss: 0.0602 - accuracy: 0.9819\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 8s 16ms/step - loss: 0.0457 - accuracy: 0.9859\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 8s 16ms/step - loss: 0.0369 - accuracy: 0.9882\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 8s 16ms/step - loss: 0.0310 - accuracy: 0.9904\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 8s 16ms/step - loss: 0.0277 - accuracy: 0.9914\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 8s 16ms/step - loss: 0.0215 - accuracy: 0.9931\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 8s 16ms/step - loss: 0.0209 - accuracy: 0.9933\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 8s 16ms/step - loss: 0.0175 - accuracy: 0.9942\n",
      "313/313 - 1s - loss: 0.0432 - accuracy: 0.9879\n",
      "Test accuracy: 0.9879000186920166\n"
     ]
    }
   ],
   "source": [
    "############# Write here. #############\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=128)\n",
    "#######################################\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOowIsLDs7J_"
   },
   "source": [
    "## Wrap-up\n",
    "\n",
    "So far, we have learned how we can define and train models in TensorFlow. For more information you can refer to [guides in TensorFlow official website](https://www.tensorflow.org/guide) and many other blog posts."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TF_high_level_apis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
