{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwlPAA8ZJUsr"
   },
   "source": [
    "Copyright (C) 2020 Software Platform Lab, Seoul National University\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); \n",
    "\n",
    "you may not use this file except in compliance with the License. \n",
    "\n",
    "You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 \n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software \n",
    "\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS, \n",
    "\n",
    "\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n",
    "\n",
    "\n",
    "See the License for the specific language governing permissions and\n",
    "\n",
    "\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ki_RHIwPJvyn"
   },
   "source": [
    "# **1. TensorFlow Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5N1npMVQqJz"
   },
   "source": [
    "## Constant Tensor\n",
    "\n",
    "Let's create a constant tensor in TensorFlow.\n",
    "\n",
    "**```tf.constant(\n",
    "    value, dtype=None, shape=None, name='Const'\n",
    ")```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EFZ3bfVsQz_p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2]\n",
      "[[0 2]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# constant of 1d tensor, or a vector\n",
    "a = tf.constant([2,2], name = 'vector')\n",
    "\n",
    "# constant of 2x2 tensor, or a matrix\n",
    "b = tf.constant([[0,2], [1,3]], name = 'matrix')\n",
    "\n",
    "print(a.numpy())\n",
    "print(b.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrE4WkZNUn9o"
   },
   "source": [
    "## Mathematical Operations\n",
    "\n",
    "The following example shows a matrix division operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "drceRvn4VGec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Print div\n",
      "[[0.   0.25]\n",
      " [1.   0.75]]\n"
     ]
    }
   ],
   "source": [
    "# Create constant tensors a and b\n",
    "a = tf.constant([2,4], name = 'a', dtype = tf.float32)\n",
    "b = tf.constant([[0,1], [2,3]], name = 'b', dtype = tf.float32)\n",
    "  \n",
    "# Execute division operation using b and a\n",
    "div = tf.divide(b, a)\n",
    "# or equivalently, div = b / a\n",
    "\n",
    "print('\\nPrint div')\n",
    "print(div.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnNhG69dgWsA"
   },
   "source": [
    "## Quiz 1\n",
    "**Create two constants with shape=[2,2] and perform matrix multiplication. Print the result using c. (HINT: use `tf.matmul`)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QZyw665-gWsE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19. 22.]\n",
      " [43. 50.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def matmul(x: list, y: list):\n",
    "    ############# Write here. ################\n",
    "    x = tf.constant(x, dtype = tf.float32)\n",
    "    y = tf.constant(y, dtype = tf.float32)\n",
    "    return tf.matmul(x, y)\n",
    "    ##########################################\n",
    "x = [[1, 2], [3, 4]]\n",
    "y = [[5, 6], [7, 8]]\n",
    "z = matmul(x, y)\n",
    "print(z.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4UKKwwsZa4I"
   },
   "source": [
    "## Variables\n",
    "\n",
    "Shared, mutable states (e.g., model parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TubsFPeZ0Rz"
   },
   "source": [
    "### Creating Variable\n",
    "\n",
    "To declare a variable, you create an instance of the class `tf.Variable`.\n",
    "\n",
    "#### Usage of TF Variable\n",
    "\n",
    "\n",
    "```\n",
    "x = tf.Variable(...)\n",
    "x.read_value()      # read value\n",
    "x.assign(...)       # x = ...\n",
    "x.assign_add(...)   # x += ...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcriAnGbaCKk"
   },
   "source": [
    "One way to create a variable is: \n",
    "\n",
    "**```tf.Variable(< initial-value >, name = < optional-name >)```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsaTk0M3hr0p"
   },
   "source": [
    "This example creates three variables using `tf.Variable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "32t-sxgsaFXh"
   },
   "outputs": [],
   "source": [
    "# Create scalar variable\n",
    "s = tf.Variable(2, name = 'scalar')\n",
    "# Create matrix variable\n",
    "m = tf.Variable([[0,1], [2,3]], name = 'matrix')\n",
    "# Create zero matrix using tf.zeros\n",
    "W = tf.Variable(tf.zeros([784,10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "O91x5GeZzW2N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s:\n",
      "2\n",
      "\n",
      "m:\n",
      "[[0 1]\n",
      " [2 3]]\n",
      "\n",
      "W:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Print values of Variable s, m and W\n",
    "print('s:')\n",
    "print(s.read_value().numpy())\n",
    "print('\\nm:')\n",
    "print(m.read_value().numpy())\n",
    "print('\\nW:')\n",
    "print(W.read_value().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "239nTB8ScEMD"
   },
   "source": [
    "### Changing values of variables\n",
    "\n",
    "To change the value of a variable, we need to assign a new value to the variable.\n",
    "You can see variable `v` changes after `assign` operations are executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Ezu6kFnScVd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v:\n",
      "[[ 2.1226544   2.8700643   2.4796543 ]\n",
      " [ 2.5823636  -0.70826054 -2.048724  ]]\n",
      "v:\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "v:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# v is a 2 x 3 variable of random values\n",
    "initializer = tf.random_normal_initializer(mean=1., stddev=2.)\n",
    "v = tf.Variable(initializer(shape=[2, 3]))\n",
    "\n",
    "# c is a 2 x 3 constant with 1.0\n",
    "c = tf.constant(1.0, shape=(2,3))\n",
    "# assign_1 = v.assign(c)\n",
    "# assign_2 = v.assign([[1., 2., 3.], [4., 5., 6.]])\n",
    "\n",
    "# Get value\n",
    "print('v:')\n",
    "print(v.read_value().numpy())\n",
    "\n",
    "# Assign new value to the variable\n",
    "v.assign(c)\n",
    "\n",
    "# Get value again\n",
    "print('v:')\n",
    "print(v.read_value().numpy())\n",
    "\n",
    "# Assign new value to the variable\n",
    "v.assign([[1., 2., 3.], [4., 5., 6.]])\n",
    "\n",
    "# Get value again\n",
    "print('v:')\n",
    "print(v.read_value().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gs3yizqwmxHs"
   },
   "source": [
    "## Quiz 2\n",
    "Define a variable (name : \"term\") with shape = [] and dtype = `tf.float64`. Initialize the variable as `2` first.\n",
    "Define another variable (name : \"sum\") with shape = [] and dtype = `tf.float64`. Initialize the variable as zeros.\n",
    "\n",
    "By using these two variables, compute the following:\n",
    "$sum = 1/term_1 + 1/term_2 + ... + 1/term_{10}$\n",
    "where\n",
    "$term_i = term_{i-1} * (term_{i-1} - 1) + 1$\n",
    "and $term_1 = 2$.\n",
    "(This recurrence relation is known as Sylvester's sequence.)\n",
    "\n",
    "Hint: Repeat updating the variables \"sum\" and \"term\" 10 times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "rhqP7g40o0z2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'sum:0' shape=() dtype=float32, numpy=1.0000001>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "term = tf.Variable(2.0, name=\"term\")\n",
    "s = tf.Variable(0.0, name=\"sum\")\n",
    "############# Write here. ################\n",
    "for _ in range(10):\n",
    "    s.assign_add(1 / term)\n",
    "    term.assign(term * (term - 1) + 1)\n",
    "\n",
    "##########################################\n",
    "\n",
    "# print('s:', s.read_value().numpy())\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbiAmxry-n75"
   },
   "source": [
    "# **2. Dataset API**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mHYbZmS06zb"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The `tf.data` API is the most advanced API for writing TensorFlow input pipelines.\n",
    "\n",
    "It allows you to build complex pipelines by composing simple building blocks. \n",
    "\n",
    "`tf.data.Dataset` is an abstraction representing a sequence of elements (each element represents one or more `tf.Tensor`s)\n",
    "\n",
    "Users can create new Datasets from existing `tf.Tensor`s by using static methods like `Dataset.from_tensor_slices()`. \n",
    "\n",
    "For example, you can create a Dataset of string Tensors that represents input file names. \n",
    "\n",
    "Transformation of exisiting Datasets is another way of creating new dataset. \n",
    "\n",
    "TensorFlow provides frequently-used Dataset transformations such as `Dataset.batch` or `Dataset.shuffle` (please refer to https://www.tensorflow.org/api_docs/python/tf/data/Dataset). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2N-qVWB4VNW"
   },
   "source": [
    "### `tf.data.Dataset.from_tensor_slices()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knXKv93E4drK"
   },
   "source": [
    "Creates a Dataset whose elements are slices of the given *python array* or *numpy array* or *tensors*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7lVbbwCfzGbx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "14\n",
      "16\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr = np.arange(10) # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Create a dataset from a numpy array\n",
    "dataset = tf.data.Dataset.from_tensor_slices(arr)\n",
    "\n",
    "# Iterate through the dataset\n",
    "for element in dataset:\n",
    "    # Print multiplied value for each element in the dataset\n",
    "    print((element * 2).numpy())\n",
    "\n",
    "# You can also use an iterator like this:\n",
    "# iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hV-LVSuslOLE"
   },
   "source": [
    "## Create a dataset from files using the Dataset API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCt7x4ByogNm"
   },
   "source": [
    "### Create dummy binary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "MTFhI2kBomkO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def create_bin_file(file_name, value):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        f.write(np.arange(value, value+4, dtype=np.int32))\n",
    "    \n",
    "bin_filenames = []\n",
    "for i in range(3):\n",
    "    file_name = 'binary_file_%d'% i\n",
    "    create_bin_file(file_name, i)\n",
    "    bin_filenames.append(file_name)\n",
    "\n",
    "# first file:\n",
    "# 0 1 2 3\n",
    "\n",
    "# second file:\n",
    "# 1 2 3 4\n",
    "\n",
    "# third file:\n",
    "# 2 3 4 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rBvsoONqBSl"
   },
   "source": [
    "### FixedLengthRecordDataset : each fixed-length slice of bytes is a dataset element.\n",
    "\n",
    "In this example, each data instance is a 8-byte integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "48JvZOGtqEwY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2, data: [0 1]\n",
      "step 2, data: [2 3]\n",
      "step 2, data: [1 2]\n",
      "step 2, data: [3 4]\n",
      "step 2, data: [2 3]\n",
      "step 2, data: [4 5]\n"
     ]
    }
   ],
   "source": [
    "# create a Dataset that contains slices (size: 8 bytes) of the files\n",
    "dataset = tf.data.FixedLengthRecordDataset(bin_filenames, 8)\n",
    "# or equivalently,\n",
    "# ds = tf.data.Dataset.from_tensor_slices(bin_filenames)\n",
    "# ds = ds.apply(lambda filename: tf.data.FixedLengthRecordDataset(filename, 8))\n",
    "\n",
    "# Iterate through the dataset\n",
    "for element in dataset:\n",
    "  # convert 8 bytes into int32 => two int32 value per each element\n",
    "  print('step %d, data: %s' % (i, tf.io.decode_raw(element, 'int32').numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5FwnpyyneZQ"
   },
   "source": [
    "### Create dummy text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BdSZO50jlSbB"
   },
   "outputs": [],
   "source": [
    "def create_text_file(file_name, index):\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write('Hello_%d\\n' % index)\n",
    "        f.write('TensorFlow_%d\\n' % index)\n",
    "\n",
    "text_filenames = []\n",
    "for i in range(3):\n",
    "    file_name = 'text_file_%d'% i\n",
    "    create_text_file(file_name, i)\n",
    "    text_filenames.append(file_name)\n",
    "\n",
    "# first file:\n",
    "# Hello_0\n",
    "# TensorFlow_0\n",
    "\n",
    "# second file:\n",
    "# Hello_1\n",
    "# TensorFlow_1\n",
    "\n",
    "# third file:\n",
    "# Hello_2\n",
    "# TensorFlow_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jv2Vc8gt9WbO"
   },
   "source": [
    "### TextLineDataset : each text line is a dataset element.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Nw5gAwbcCNPm"
   },
   "outputs": [],
   "source": [
    "def iterate_and_print(iterator, count=6):\n",
    "    for i in range(count):\n",
    "        print('step %d, data: %s' % (i, next(iterator).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "hYY0TdipmvoU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, data: b'Hello_0'\n",
      "step 1, data: b'TensorFlow_0'\n",
      "step 2, data: b'Hello_1'\n",
      "step 3, data: b'TensorFlow_1'\n",
      "step 4, data: b'Hello_2'\n",
      "step 5, data: b'TensorFlow_2'\n"
     ]
    }
   ],
   "source": [
    "# create a Dataset that contains each line of the text files\n",
    "ds = tf.data.TextLineDataset(text_filenames)\n",
    "# or equivalently,\n",
    "# ds = tf.data.Dataset.from_tensor_slices(text_filenames)\n",
    "# ds = ds.apply(lambda filename: tf.data.TextLineDataset(filename))\n",
    "\n",
    "# Create iterator for the dataset\n",
    "iterator = iter(ds)\n",
    "\n",
    "iterate_and_print(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8eqeLjir-ar"
   },
   "source": [
    "## Transform dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GinK40Kp5jQA"
   },
   "source": [
    "**ds.shuffle(buffer_size)**\n",
    "\n",
    "shuffle: shuffle data instances randomly. buffer size represents the number of data instances to be sampled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzImNht48XKW"
   },
   "source": [
    "`ds.shuffle` with N > 1 can pick data instances randomly from the buffer containing N instances. The code snippet below shows that we always do not get the 5th or 6th element of the dataset (Hello_2 or TensorFlow_2) at step 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "b3oaSOHVsGLc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, data: b'Hello_0'\n",
      "step 1, data: b'Hello_2'\n",
      "step 2, data: b'TensorFlow_2'\n",
      "step 3, data: b'TensorFlow_0'\n",
      "step 4, data: b'Hello_1'\n",
      "step 5, data: b'TensorFlow_1'\n"
     ]
    }
   ],
   "source": [
    "# Load the text file created previously\n",
    "ds = tf.data.TextLineDataset(text_filenames) \n",
    "# shuffle the dataset using buffer size 4\n",
    "ds = ds.shuffle(4)\n",
    "\n",
    "iterator = iter(ds)\n",
    "iterate_and_print(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bP-MH37I8s8z"
   },
   "source": [
    "`ds.shuffle` with N == 1 has no shuffling effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "c8Q9W2pG6nxF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, data: b'Hello_0'\n",
      "step 1, data: b'TensorFlow_0'\n",
      "step 2, data: b'Hello_1'\n",
      "step 3, data: b'TensorFlow_1'\n",
      "step 4, data: b'Hello_2'\n",
      "step 5, data: b'TensorFlow_2'\n"
     ]
    }
   ],
   "source": [
    "# Load the text file created previously\n",
    "ds = tf.data.TextLineDataset(text_filenames)\n",
    "# shuffle the dataset using buffer size 1\n",
    "ds = ds.shuffle(1)\n",
    "\n",
    "iterator = iter(ds)\n",
    "iterate_and_print(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9i-rJwnE6Vdj"
   },
   "source": [
    "**ds.repeat(count)**\n",
    "\n",
    "Repeat the data instances count times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WM9xpfjr72Qo"
   },
   "source": [
    "An error is raised when an iterator calls a next element after reading all the data from the dataset. `ds.repeat(count)` repeats the dataset `ds` so each original value is seen `count` times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "KUzyWsUt6WQ_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, data: b'Hello_0'\n",
      "step 1, data: b'TensorFlow_0'\n",
      "step 2, data: b'Hello_1'\n",
      "step 3, data: b'TensorFlow_1'\n",
      "step 4, data: b'Hello_2'\n",
      "step 5, data: b'TensorFlow_2'\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    745\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2727\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2728\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2729\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6940\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6941\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6942\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence [Op:IteratorGetNext]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-daa87c0588ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0miterate_and_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-92b2584a4fa9>\u001b[0m in \u001b[0;36miterate_and_print\u001b[0;34m(iterator, count)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0miterate_and_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step %d, data: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ds = tf.data.TextLineDataset(text_filenames)\n",
    "\n",
    "iterator = iter(ds)\n",
    "iterate_and_print(iterator, count=7) # error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_CwPo5I76G9"
   },
   "source": [
    "`ds.repeat(count)` repeats iterating the dataset `count` times. If we do not pass the `count` argument, the dataset repeats forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "FzqJY23X8Hg0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, data: b'Hello_0'\n",
      "step 1, data: b'TensorFlow_0'\n",
      "step 2, data: b'Hello_1'\n",
      "step 3, data: b'TensorFlow_1'\n",
      "step 4, data: b'Hello_2'\n",
      "step 5, data: b'TensorFlow_2'\n",
      "step 6, data: b'Hello_0'\n",
      "step 7, data: b'TensorFlow_0'\n",
      "step 8, data: b'Hello_1'\n",
      "step 9, data: b'TensorFlow_1'\n",
      "step 10, data: b'Hello_2'\n",
      "step 11, data: b'TensorFlow_2'\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    745\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2727\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2728\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2729\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6940\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6941\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6942\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence [Op:IteratorGetNext]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-0f478b6c7a41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0miterate_and_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-92b2584a4fa9>\u001b[0m in \u001b[0;36miterate_and_print\u001b[0;34m(iterator, count)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0miterate_and_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step %d, data: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ds = tf.data.TextLineDataset(text_filenames)\n",
    "\n",
    "# repeat twice\n",
    "ds = ds.repeat(2)\n",
    "\n",
    "iterator = iter(ds)\n",
    "iterate_and_print(iterator, count=13) # error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yM9UYyhdamR"
   },
   "source": [
    "Another common pattern is to use try-except clause to detect the end of epoch. Once we finisn an epoch, we re-initialize the iterator to start from the beginning again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "NyHGXUxadamS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, data: b'Hello_0'\n",
      "step 1, data: b'TensorFlow_0'\n",
      "step 2, data: b'Hello_1'\n",
      "step 3, data: b'TensorFlow_1'\n",
      "step 4, data: b'Hello_2'\n",
      "step 5, data: b'TensorFlow_2'\n",
      "Finished epoch 0\n",
      "step 6, data: b'Hello_0'\n",
      "step 7, data: b'TensorFlow_0'\n",
      "step 8, data: b'Hello_1'\n",
      "step 9, data: b'TensorFlow_1'\n",
      "step 10, data: b'Hello_2'\n",
      "step 11, data: b'TensorFlow_2'\n",
      "Finished epoch 1\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.TextLineDataset(text_filenames)\n",
    "\n",
    "iterator = iter(ds)\n",
    "\n",
    "epoch = 0\n",
    "step = 0\n",
    "\n",
    "while True:\n",
    "    # repeat until we detect an error\n",
    "    try:\n",
    "        v = next(iterator).numpy()\n",
    "        print('step %d, data: %s' % (step, v))\n",
    "        step += 1\n",
    "    # iterator raises StopIteration once we finish an epoch\n",
    "    except StopIteration:\n",
    "        print('Finished epoch', epoch)\n",
    "        epoch += 1\n",
    "        # if we are done with 2 epochs, break\n",
    "        if epoch >= 2:\n",
    "            break\n",
    "        # otherwise, re-create an iterator\n",
    "        iterator = iter(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71CqxItI4_Rk"
   },
   "source": [
    "**ds.batch(batch_size)**\n",
    "\n",
    "Combines elements of this dataset into batches. `batch_size` represents the number of data instances to combine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "8stEmpyxvT18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, data: [b'Hello_0' b'TensorFlow_0' b'Hello_1']\n",
      "step 1, data: [b'TensorFlow_1' b'Hello_2' b'TensorFlow_2']\n",
      "step 2, data: [b'Hello_0' b'TensorFlow_0' b'Hello_1']\n",
      "step 3, data: [b'TensorFlow_1' b'Hello_2' b'TensorFlow_2']\n",
      "step 4, data: [b'Hello_0' b'TensorFlow_0' b'Hello_1']\n",
      "step 5, data: [b'TensorFlow_1' b'Hello_2' b'TensorFlow_2']\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.TextLineDataset(text_filenames) \n",
    "# batch elements using batch_size 3\n",
    "ds = ds.batch(3)\n",
    "ds = ds.repeat(3)\n",
    "\n",
    "iterator = iter(ds)\n",
    "iterate_and_print(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meX0yDAd4KCD"
   },
   "source": [
    "**ds.map(fn)**\n",
    "\n",
    "Apply `fn` to each element of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "zITLEwgys7l8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, data: [b'Hello_0+TensorFlow_0+Hello_1']\n",
      "step 1, data: [b'TensorFlow_1+Hello_2+TensorFlow_2']\n",
      "step 2, data: [b'Hello_0+TensorFlow_0+Hello_1']\n",
      "step 3, data: [b'TensorFlow_1+Hello_2+TensorFlow_2']\n",
      "step 4, data: [b'Hello_0+TensorFlow_0+Hello_1']\n",
      "step 5, data: [b'TensorFlow_1+Hello_2+TensorFlow_2']\n"
     ]
    }
   ],
   "source": [
    "# split the `data` tensor into 3 pieces and concatenate the pieces by inserting '+' between them\n",
    "def split_join(data):\n",
    "    data = tf.split(data, 3)\n",
    "    return tf.strings.join(data, '+')\n",
    "\n",
    "ds = tf.data.TextLineDataset(text_filenames)\n",
    "ds = ds.batch(3)\n",
    "ds = ds.repeat(3)\n",
    "ds = ds.map(split_join)\n",
    "\n",
    "iterator = iter(ds)\n",
    "iterate_and_print(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXqUUZeo5a9E"
   },
   "source": [
    "##  Speed up Dataset processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WplDJMqQ59im"
   },
   "source": [
    "\n",
    "**ds.interleave(map_func, cycle_length)**\n",
    "\n",
    "map_func : map function to apply to each data instance\n",
    "\n",
    "cycle_length : the number of data instances to process concurrently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elNSpQlF9Kkd"
   },
   "source": [
    "We can use this feature to read and process multiple files concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "dlIOImd6KHHr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, data: b'Hello_0'\n",
      "step 1, data: b'Hello_1'\n",
      "step 2, data: b'TensorFlow_0'\n",
      "step 3, data: b'TensorFlow_1'\n",
      "step 4, data: b'Hello_2'\n",
      "step 5, data: b'TensorFlow_2'\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(text_filenames)\n",
    "# consume the first two files in concurrently, and then the third file \n",
    "ds = ds.interleave(lambda filename: tf.data.TextLineDataset(filename),\n",
    "                    cycle_length=2)\n",
    "\n",
    "iterator = iter(ds)\n",
    "iterate_and_print(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhp0R08AKxpy"
   },
   "source": [
    "**ds.prefetch(buffer_size)** \n",
    "\n",
    "prefetch elements from a dataset. buffer size represents the maximum buffer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "AWM2PH6SLHrb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, data: b'Hello_0'\n",
      "step 1, data: b'Hello_1'\n",
      "step 2, data: b'Hello_2'\n",
      "step 3, data: b'TensorFlow_0'\n",
      "step 4, data: b'TensorFlow_1'\n",
      "step 5, data: b'TensorFlow_2'\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(text_filenames)\n",
    "ds = ds.interleave(lambda filename: tf.data.TextLineDataset(filename),\n",
    "                    cycle_length=3)\n",
    "ds = ds.prefetch(3)\n",
    "\n",
    "iterator = iter(ds)\n",
    "iterate_and_print(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEs3qEJszYgu"
   },
   "source": [
    "## Quiz 3\n",
    "Create a dataset following the instructions.\n",
    "\n",
    "1. Create a textline dataset using files named `ex_filenames`. \n",
    "2. Shuffle the dataset with buffer size 15.\n",
    "3. Repeat the dataset for 2 epochs.\n",
    "4. Convert each data instance using the `cast` function defined below.\n",
    "5. Make the data instances as a batch (batch size = 3).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "C_TOoWhx1O2S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, data: [4.1 4.  3. ]\n",
      "step 1, data: [1.1 0.2 2. ]\n",
      "step 2, data: [3.1 2.2 3.2]\n",
      "step 3, data: [2.1 1.2 1. ]\n",
      "step 4, data: [0.  4.2 0.1]\n",
      "step 5, data: [4.1 1.  0. ]\n",
      "step 6, data: [1.1 2.2 0.1]\n",
      "step 7, data: [1.2 2.1 3. ]\n",
      "step 8, data: [3.2 0.2 2. ]\n",
      "step 9, data: [4.2 3.1 4. ]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def create_text_file(index):\n",
    "    with open('ex_file_%d'%index, 'w') as f:\n",
    "        for i in range(3):\n",
    "            f.write('%d.%d\\n' % (index, i))\n",
    "    \n",
    "ex_filenames = []\n",
    "for i in range(5):\n",
    "    create_text_file(i)\n",
    "    ex_filenames.append('ex_file_%d'% i)\n",
    "\n",
    "def cast(data):\n",
    "    data = tf.strings.to_number(data, out_type=tf.float32)\n",
    "    return data\n",
    "\n",
    "\n",
    "############# Write here. ################\n",
    "\n",
    "# Create a Dataset\n",
    "ds = tf.data.TextLineDataset(ex_filenames)\n",
    "# Shuffle\n",
    "ds = ds.shuffle(15)\n",
    "# Repeat\n",
    "ds = ds.repeat(2)\n",
    "# Transformation\n",
    "ds = ds.map(cast)\n",
    "# Create a mini-batch\n",
    "ds = ds.batch(3)\n",
    "\n",
    "##########################################\n",
    "\n",
    "iterator = iter(ds)\n",
    "iterate_and_print(iterator, count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SO0i-ksu8jB9"
   },
   "source": [
    "# **FYI: TensorFlow v1 vs. TensorFlow v2** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDH6BhQ3P7ou"
   },
   "source": [
    "Throughout the tutorial, we used the latest release of TensorFlow. Check out the version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "fn3qNA6uP7BY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thHFaK0L8nqp"
   },
   "source": [
    "Previously in TensorFlow v1, we construct a graph using **Graph** which consists of TensorFlow operations (Ops). \n",
    "\n",
    "After defining a Graph, we could run it via **Session**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22S2dhxtqV1m"
   },
   "outputs": [],
   "source": [
    "# graph = tf.Graph()\n",
    "# with graph.as_default():\n",
    "      # Dataset\n",
    "      # Build a model\n",
    "      # Training\n",
    "#     // Here we load dataset, define operations, etc.\n",
    "\n",
    "# with tf.Session(graph=graph) as sess:\n",
    "#     sess.run(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXypoQu4Qn53"
   },
   "source": [
    "Switching from TensorFlow v1 to TensorFlow v2, there have been many things changed.\n",
    "To sum up the update, TensorFlow shifted to **eager execution** (imperative execution) by default. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipQBjqopUOWM"
   },
   "source": [
    "Eager execution provides an intuitive interface to structure the code naturally and use Python data structures. It is also easier to debug and test changes by using standard Python debugging tools.\n",
    "Lastly, it has natural Python control flow instead of graph control flow, simplifying the specification of dynamic models. The downside is that eager execution is much slower than symbolic execution, the default TensorFlow v1 mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXLES0lRVuVJ"
   },
   "source": [
    "Let's see the difference of the two versions with an example of dynamic control flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vwwvlb_7V4Fo"
   },
   "source": [
    "## Dynamic control flow\n",
    "A major benefit of eager execution is that all the functionality of the host language is available while your model is executing. So, for example, it is easy to write fizzbuzz game where any number divisible by three is replaced with the word \"fizz\", and any number divisible by five is replaced with the word \"buzz\" (similar to the 3-6-9 game)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhfCip_FV1j3"
   },
   "outputs": [],
   "source": [
    "# Native python code\n",
    "def fizzbuzz(max_num):\n",
    "    counter = 0\n",
    "    for num in range(1, max_num+1):\n",
    "        if int(num % 3) == 0 and int(num % 5) == 0:\n",
    "            print('FizzBuzz')\n",
    "        elif int(num % 3) == 0:\n",
    "            print('Fizz')\n",
    "        elif int(num % 5) == 0:\n",
    "            print('Buzz')\n",
    "        else:\n",
    "            print(num)\n",
    "        counter += 1\n",
    "    \n",
    "fizzbuzz(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgAzIPm5WJ8R"
   },
   "source": [
    "In eager execution, we need to add minor changes \n",
    "in a few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIb2eV37WOPX"
   },
   "outputs": [],
   "source": [
    "def fizzbuzz_eager(max_num):\n",
    "    counter = tf.constant(0) # counter = 0\n",
    "    max_num = tf.convert_to_tensor(max_num) #\n",
    "    for num in range(1, max_num.numpy() + 1): #\n",
    "        num = tf.constant(num) # \n",
    "        if int(num % 3) == 0 and int(num % 5) == 0:\n",
    "            print('FizzBuzz')\n",
    "        elif int(num % 3) == 0:\n",
    "            print('Fizz')\n",
    "        elif int(num % 5) == 0:\n",
    "            print('Buzz')\n",
    "        else:\n",
    "            print(num.numpy()) # print(num)\n",
    "        counter += 1\n",
    "    \n",
    "fizzbuzz_eager(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QGYXcyyWRzN"
   },
   "source": [
    "Implementing the same thing using graph mode in TensorFlow v1: [Tensorflow FizzBuzz Revisited (Ricky Han blog)](https://rickyhan.com/jekyll/update/2018/02/16/tensorflow-fizzbuzz-revisited.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QpFmaoDWaRU"
   },
   "outputs": [],
   "source": [
    "# This code does not run in tensorflow 2.x\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def fizzbuzz_graph(max_num):\n",
    "    # Define variable and while_loop\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        arr = tf.Variable([str(i) for i in range(1, max_num+1)])\n",
    "        # nasty tf.while_loop and tf.cond ops\n",
    "        while_op = tf.while_loop(\n",
    "            (lambda i, _: tf.less(i, max_num+1)), \n",
    "            (lambda i, _: (tf.add(i,1), tf.cond(\n",
    "                tf.logical_and(tf.equal(tf.mod(i, 3), 0), tf.equal(tf.mod(i, 5), 0)),\n",
    "                (lambda : tf.assign(arr[(i - 1)], 'FizzBuzz')),\n",
    "                (lambda : tf.cond(tf.equal(tf.mod(i, 3), 0),\n",
    "                    (lambda : tf.assign(arr[(i - 1)], 'Fizz')),\n",
    "                    (lambda : tf.cond(tf.equal(tf.mod(i, 5), 0),\n",
    "                        (lambda : tf.assign(arr[(i - 1)], 'Buzz')),\n",
    "                        (lambda : arr)))))))),\n",
    "            [1, arr])\n",
    "\n",
    "    # Call Session.run()\n",
    "    with tf.Session(graph = graph) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        idx, array = sess.run(while_op)\n",
    "        print(array)\n",
    "\n",
    "\n",
    "fizzbuzz_graph(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10w0TYJN8p5L"
   },
   "source": [
    "TensorFlow2 can still benefit from graph-based execution as it provides `tf.function` where we can define a function with operations. TensorFlow constructs a graph for the function automatically and apply possible optimizations.\n",
    "\n",
    "For more information on `tf.function`, refer to this website: https://www.tensorflow.org/guide/function?hl=ko"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TF_basic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
