{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ec1NCtLafD2"
   },
   "source": [
    "# 1. Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFpwJWt6H8i-"
   },
   "source": [
    "### 1-1. Import Libraries\n",
    "- 데이터셋 다운로드와 전처리를 쉽게 하는 torchtext 라이브러리를 import 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "fuWiZzTJIHlX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchtext.legacy import data, datasets\n",
    "\n",
    "import random\n",
    "import time\n",
    "import spacy\n",
    "import numpy as np\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlBRh_erIV5Y"
   },
   "source": [
    "### 1-2. Load data\n",
    "- Field 를 정의합니다.\n",
    "- IMDB 데이터를 다운받습니다.\n",
    "- Train, Valid, Test 데이터셋으로 split 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "1qAjGhs_momr"
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(include_lengths=True)\n",
    "\n",
    "# 가변 길이가 아닌 고정된 길이의 seqeunce 로 데이터를 입력 받고 싶을 때?\n",
    "'''\n",
    "MAX_LEN = 20\n",
    "TEXT = data.Field(fix_length=MAX_LEN)\n",
    "'''\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lAyLqGf_sOgf",
    "outputId": "96cea62c-848e-408c-e3ae-9dc924edba40"
   },
   "outputs": [],
   "source": [
    "# Download IMDB data\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "xhhfnEZYsld1"
   },
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "9dDuDBdvsOmk"
   },
   "outputs": [],
   "source": [
    "# Split train and valid data\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NdLyvaV6o-TY",
    "outputId": "3be37625-5997-47fb-c5a0-19620c9ffe19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print('Number of training examples: {}'.format(len(train_data)))\n",
    "print('Number of validation examples: {}'.format(len(valid_data)))\n",
    "print('Number of testing examples: {}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEsf_UifafEH"
   },
   "source": [
    "### 1-3. Cuda Setup\n",
    "- GPU 사용을 위한 Cuda 설정\n",
    "- Colab 페이지 상단 메뉴>수정>노트설정에서 GPU 사용 설정이 선행되어야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "W4BsnajZafEI"
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8fuDdBOafEO",
    "outputId": "a05e75ab-3506-4c15-b8cf-652b014748c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov  5 17:39:08 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 166...  Off  | 00000000:0A:00.0  On |                  N/A |\r\n",
      "|  0%   50C    P5    13W / 125W |   5117MiB /  5941MiB |      5%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1209      G   /usr/lib/xorg/Xorg                 59MiB |\r\n",
      "|    0   N/A  N/A      1763      G   /usr/lib/xorg/Xorg                266MiB |\r\n",
      "|    0   N/A  N/A      1910      G   /usr/bin/gnome-shell               34MiB |\r\n",
      "|    0   N/A  N/A      4103      G   /opt/zoom/zoom                     59MiB |\r\n",
      "|    0   N/A  N/A      4184      G   ...AAAAAAAAA= --shared-files       53MiB |\r\n",
      "|    0   N/A  N/A     13611      C   ...endo/anaconda3/bin/python     4607MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kC9xHrlt4LL9"
   },
   "source": [
    "##2. Preprocess data\n",
    "- Vocab (단어장) 을 만듭니다.\n",
    "- Iterator 를 만듭니다. (Iterator 를 통해 batch training 을 위한 batching 과 padding, 그리고 데이터 내 단어들의 인덱스 변환이 이루어집니다.)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "suJRz9fgur1_",
    "outputId": "6ab7c7ea-e510-401c-c663-635737b3f917"
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(train_data,\n",
    "                 max_size = MAX_VOCAB_SIZE,\n",
    "                 vectors = \"glove.6B.100d\",\n",
    "                 unk_init = torch.Tensor.normal_                 \n",
    "                 )\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ES2qQ9kdu71e",
    "outputId": "8f2f64e0-211d-497a-b669-f94b09fb182c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "sOD6AiIwKu-G"
   },
   "outputs": [],
   "source": [
    "# Batching - construct iterator\n",
    "BATCH_SIZE = 1\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_sizes = (BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_tAoIcUMp3v"
   },
   "source": [
    "##3. Build Model\n",
    "- Embedding layer, Transformer layer, Fully-connected layer 로 이루어진 모델을 만듭니다.\n",
    "- Classification task 에 활용하기 위해 기존 Seq2Seq Transformer 를 변형하여, Transformer Encoder 만을 활용합니다.\n",
    "- Positional Encoding 식\n",
    "> $PE_(pos,2i) =sin(pos/10000^{2i/d_{model}})$  \n",
    "> $PE_(pos,2i+1) =cos(pos/10000^{2i/d_{model}})$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "XB9WQX3yrC2P"
   },
   "outputs": [],
   "source": [
    "class PositionalEnc(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJqioqW_c9jU"
   },
   "source": [
    "## HOMEWORK (2-1) 포함\n",
    "- 데이터를 고정된 길이로 설정합니다. (위의 Load data 파트에서 확인)\n",
    "- PositionEnc 클래스를 참조하여 max_len 인자를 전달할 수 있도록 TransformerNet 클래스를 수정합니다.\n",
    "- 마지막 prediction을 위해 사용되는 입력값으로 encoder의 마지막 token만 취하도록 바꿔줍니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "6slAdlS5giEY"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "HOMEWORK(2-1):\n",
    "실습 시간에 작성한 코드는 Transformer 가 출력한 전체 sequence representation의 평균값을 pooled 로 받도록 되어있습니다.\n",
    "과제에서는 TransformerNet 이 가변 길이의 데이터가 아닌 고정 길이의 데이터만 처리하도록 변경하고\n",
    "sequence의 마지막 1개의 token의 representation 을 pooled 로 취하도록 코드를 수정해주세요.\n",
    "(hint) max_len 전달을 위해 TransformerNet 클래스에 파라미터를 추가해야합니다.\n",
    "'''\n",
    "\n",
    "class TransformerNet(nn.Module):    \n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_heads, n_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define parameters\n",
    "        self.hidden_him = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Define Layers\n",
    "        # TO-DO Embedding layer\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.pe =  PositionalEnc(embedding_dim)\n",
    "      \n",
    "        '''\n",
    "        HOMEWORK (2-1) hint\n",
    "        PositionalEnc 클래스의 max_len에 전달될 값을 명시해줍니다.\n",
    "        '''\n",
    "\n",
    "        # Encoder layer\n",
    "        enc_layer = nn.TransformerEncoderLayer(embedding_dim, n_heads, hidden_dim, dropout=dropout)\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "      \n",
    "        # TO-DO Fully connected layer and Dropout layer\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text = [sent len, batch size]\n",
    "\n",
    "        # TO-DO Transformer 가 출력한 output 의 평균값을 Dropout, FC layer 을 통과하여 반환\n",
    "        # hint\n",
    "        # embedded : [sent len, batch size, emb dim]\n",
    "        # trans_out : [sent len, batch_size, emb_dim]\n",
    "        # pooled : transformer 출력의 평균\n",
    "        # final : Dropout -> FC 통과한 logits\n",
    "\n",
    "        embedded =  self.embedding(text)\n",
    "        pos_encoded = self.pe(embedded)\n",
    "        trans_out = self.encoder(pos_encoded)       \n",
    "        pooled = trans_out.mean(0)\n",
    "        final = self.fc(self.dropout(pooled))\n",
    "\n",
    "        '''\n",
    "        HOMEWORK (2-1)\n",
    "        이를 sequence의 마지막 1개의 token만 취하도록 코드를 수정해보자.\n",
    "        아래 last 변수가 마지막 token을 취할 수 있게 써주시면 됩니다.\n",
    "        실습에서 작성한 pooled, final 은 주석처리해야합니다.\n",
    "\n",
    "        pooled = \n",
    "        final = self.fc(self.dropout(last))\n",
    "        '''\n",
    "\n",
    "        return final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "9fTDWd8RhEEl"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 1\n",
    "N_HEADS = 2  #embedding_dim must be divisible by n_heads\n",
    "N_LAYERS = 1\n",
    "DROPOUT = 0.5\n",
    "\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "'''\n",
    "HOMEWORK (2-1) hint\n",
    "위에서 바뀐 TransformerNet의 정의에 따라 아래 TransformerNet 의 인자를 추가해야합니다.\n",
    "'''\n",
    "\n",
    "model = TransformerNet(INPUT_DIM,      \n",
    "                       EMBEDDING_DIM, \n",
    "                       HIDDEN_DIM, \n",
    "                       OUTPUT_DIM, \n",
    "                       N_HEADS,\n",
    "                       N_LAYERS, \n",
    "                       DROPOUT,\n",
    "                       PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCUKXn94afEf",
    "outputId": "069bc2cc-c11a-402a-c547-b48befa4812e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,566,929 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('The model has {:,} trainable parameters'.format(count_parameters(model)))\n",
    "\n",
    "# load pretrained embeddings\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDy-PGhSQcTd"
   },
   "source": [
    "## 4. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "s7-5uM9bhTe3"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "model= model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "7exIxGaZafE4"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "1Eb16jU0afFA"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # TO-DO\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    for batch in iterator:\n",
    "        # TO-DO\n",
    "        # General Training Scheme\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(batch.text[0]).squeeze(1)\n",
    "        loss = criterion(prediction, batch.label)\n",
    "        acc = binary_accuracy(prediction, batch.label)\n",
    "        # Backward-pass\n",
    "        loss.backward()\n",
    "        # Backprop\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "8da_fbPRQ9I3"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            # TO-DO\n",
    "            # General Evaluation Scheme\n",
    "            prediction = model(batch.text[0]).squeeze(1)\n",
    "            loss = criterion(prediction, batch.label)\n",
    "            acc = binary_accuracy(prediction, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZhS9Q2fSLyr"
   },
   "source": [
    "### *Do Training!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K46LhrQ7012J",
    "outputId": "8eb7e387-c147-43d5-e07f-b3c4e3861530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.615 | Train Acc: 63.42%\n",
      "\t Val. Loss: 0.677 |  Val. Acc: 76.09%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.434 | Train Acc: 80.18%\n",
      "\t Val. Loss: 0.749 |  Val. Acc: 79.65%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.375 | Train Acc: 83.45%\n",
      "\t Val. Loss: 0.615 |  Val. Acc: 80.99%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.341 | Train Acc: 85.58%\n",
      "\t Val. Loss: 0.590 |  Val. Acc: 83.84%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.303 | Train Acc: 87.45%\n",
      "\t Val. Loss: 0.580 |  Val. Acc: 84.24%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.279 | Train Acc: 88.65%\n",
      "\t Val. Loss: 0.581 |  Val. Acc: 85.01%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.253 | Train Acc: 90.09%\n",
      "\t Val. Loss: 0.669 |  Val. Acc: 84.01%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 7\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    if valid_loss < best_valid_loss: # For early stopping\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'trans-model.pt') # Save the model\n",
    "    \n",
    "    print('Epoch: {:02}'.format(epoch+1))\n",
    "    print('\\tTrain Loss: {:.3f} | Train Acc: {:.2f}%'.format(train_loss, train_acc*100))\n",
    "    print('\\t Val. Loss: {:.3f} |  Val. Acc: {:.2f}%'.format(valid_loss, valid_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "En4SX_RA02k4",
    "outputId": "a952ae85-37b6-4805-b762-050a4afc300b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.583 | Test Acc: 83.45%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('trans-model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print('Test Loss: {:.3f} | Test Acc: {:.2f}%'.format(test_loss, test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EtTQaIdafFL"
   },
   "source": [
    "## 5. Test model\n",
    "우리가 직접 예문을 작성해서 트레인된 모델에서 예문을 어떻게 평가하는지 확인합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "_hxev65tafFQ"
   },
   "outputs": [],
   "source": [
    "# 토크나이저로 spacy 를 사용합니다.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# 사용자가 입력한 sentence 를 훈련된 모델에 넣었을때의 결과값을 확인합니다.\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]  # Tokenization\n",
    "    print(tokenized)\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]   # 위에서 만든 vocab 에 부여된 index 로 indexing\n",
    "    print(indexed)\n",
    "    tensor = torch.LongTensor(indexed).to(device)   # indexing 된 sequence 를 torch tensor 형태로 만들어줌.\n",
    "    print(tensor.shape)\n",
    "    tensor = tensor.unsqueeze(1)   # 입력 텐서에 batch 차원을 만들어줌.\n",
    "    prediction = torch.sigmoid(model(tensor))  # 모델에 입력한 후 확률값 도출을 위한 sigmoid 적용 \n",
    "    return prediction.item() # prediction 값 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dIk_EjmoSFdU",
    "outputId": "d4444dc0-aec2-4066-816a-c6765f2c5eac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'film', 'is', 'terrible']\n",
      "[49, 23, 7, 538]\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0006696248310618103"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is terrible\") #아주 낮은 값의 확률이 도출되는 것을 확인할 수 있습니다.(부정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A1ZZJbxaSFaa",
    "outputId": "b637b204-26a7-4d0a-d985-85ff5a107952"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'film', 'is', 'great']\n",
      "[49, 23, 7, 97]\n",
      "torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999982118606567"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"This film is great\") #아주 높은 값의 확률이 도출되는 것을 확인할 수 있습니다. (긍정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TR5d2SxlYUI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wahPDnJKka-L"
   },
   "source": [
    "# Homework (2-2) \n",
    "## GRU + Self-attention Model for Text Classification\n",
    "- Attention mechanism 을 통해 마지막 순서와 각 순서와의 관계 고려하기.\n",
    "- (1) Embedding layer\n",
    "- (2) GRU layer\n",
    "- (3) Self-attention layer\n",
    "- (4) Dropout layer\n",
    "- (5) Fully-connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "dr2tzUxake0Z"
   },
   "outputs": [],
   "source": [
    "class Self_Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.scale = 1. / math.sqrt(hidden_dim)\n",
    "\n",
    "    def forward(self, query, key, value): \n",
    "        # query == hidden: = [N, H]\n",
    "        # key/value == output: [L, N, H]\n",
    "\n",
    "        ### Key 텐서의 크기를 재배열 ###\n",
    "        query = query.unsqueeze(1) # [N, 1, H]\n",
    "        key = key.permute(1,2,0)  # [N, H, L]  \n",
    "\n",
    "        # bmm: batch matrix-matrix multiplication\n",
    "        # Query 값과 key 의 각 토큰값 간의 Dot-product\n",
    "        attention_weight = torch.bmm(query, key)  # [N, 1, L] \n",
    "\n",
    "        # Scale and normalize\n",
    "        attention_weight = F.softmax(attention_weight.mul_(self.scale), dim=2) # [N, 1, L]\n",
    "\n",
    "        # Attention weight 과 value 곱해서 attention output 구하기\n",
    "        # Value 텐서의 크기를 재배열\n",
    "        value =  value.transpose(0,1)            # [N, L, H]\n",
    "        attention_output = torch.bmm(attention_weight, value)   # [N, 1, H]\n",
    "        attention_output = attention_output.squeeze(1) # [N, H]\n",
    "\n",
    "        return attention_output, attention_weight.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "4Tt86vDfkzLg"
   },
   "outputs": [],
   "source": [
    "# Make Custom GRU + Self-attention Model\n",
    "\n",
    "class CustomModel(nn.Module): \n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define parameters\n",
    "        self.hidden_him = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Define Layers\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attention = Self_Attention(hidden_dim)\n",
    " \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        '''\n",
    "        Homework (2-2)\n",
    "        Apply GRU and Dropout, then Self-Attention\n",
    "        Use the last hidden representation of the sequence\n",
    "        hint: \n",
    "        1) self.gru(inputs=XX) Remind that gru layer returns a tuple\n",
    "        2) self.attention(query=XX, key=XX, value=XX)\n",
    "        3) self.dropout(XX) Remind that input the last hidden representation \n",
    "        '''\n",
    "        gru = self.gru(embedded)\n",
    "        hidden = self.attention(gru)\n",
    "        rescaled_hidden = self.dropout(hidden)\n",
    "\n",
    "        return self.fc(rescaled_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "u3cRVpYKk3g5"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.1\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = CustomModel(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS,\n",
    "            DROPOUT, \n",
    "            PAD_IDX)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdIjZr_bk_er",
    "outputId": "847bc099-a66f-489d-8f3e-1ed932788c97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,687,721 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# CustomModel 의 파라미터 수 확인\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)  # Count number of elements of all parameters\n",
    "\n",
    "print('The model has {:,} trainable parameters'.format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0BJywNblA1S",
    "outputId": "72d85f8c-3230-40fa-a105-5a6797831fc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# load pretrained embeddings\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "print(type(pretrained_embeddings))\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "AZ5AXWYVlChG"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())   # Gradient Descent 를 실행할 optimizer 정의\n",
    "criterion = nn.BCEWithLogitsLoss()  # 손실함수 정의 \n",
    "model = model.to(device)  #모델을 GPU 로 이동\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "NLl7qbsPlPdx"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'key' and 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-88f329ff5efb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-ee52fd60ade3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, criterion)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# General Training Scheme\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-a80a7b4783f6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     36\u001b[0m         '''\n\u001b[1;32m     37\u001b[0m         \u001b[0mgru\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mrescaled_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'key' and 'value'"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf') # Represents infinity\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    if valid_loss < best_valid_loss: # For early stopping\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'rnn-model.pt')\n",
    "    else:\n",
    "      break\n",
    "    \n",
    "    print('Epoch: {:02}'.format(epoch+1))\n",
    "    print('\\tTrain Loss: {:.3f} | Train Acc: {:.2f}%'.format(train_loss, train_acc*100))\n",
    "    print('\\t Val. Loss: {:.3f} |  Val. Acc: {:.2f}%'.format(valid_loss, valid_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3i1vM-alU05"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('rnn-model.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print('Test Loss: {:.3f} | Test Acc: {:.2f}%'.format(test_loss, test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ByA4BD62lljP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Fall2021_IAB_학생용_Transformer_practice.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
